   #O'Reilly Conferences: Web 2.0 Summit 2009

   Web 2.0 Summit
     * Home
     * Conference
          + Web 2.0 Summit Schedule
          + Speakers
          + Speaker Presentation Files
          + Events
          + Sponsored Event Lounges
          + Web Squared Charities
          + About Web 2.0 Summit
     * Sponsors
          + Web 2.0 Summit Sponsors
          + Media Sponsors
          + Sponsorship Opportunities
     * Press & Media
          + Press Info
          + PR Opportunities
          + News & Coverage
     * More Web 2.0
          + Web Squared White Paper
          + External Links
          + Web 2.0 Summit Blog
          + Select 2009 Keynote Video
          + Web 2.0 Summit 2008
          + Select 2008 Keynote Video
          + Web 2.0 Expo
          + O'Reilly Conferences
          + TechWeb Live Events
     * Connect
          + Contact Us
          + External Links
          + Web 2.0 Expo Blog
          + O'Reilly Radar

Web Squared: Web 2.0 Five Years On

   By Tim O’Reilly and John Battelle

   Download the Web Squared White Paper (PDF, 1.3MB)

   Watch the Web Squared Webcast

   Five years ago, we launched a conference based on a simple idea, and
   that idea grew into a movement. The original Web 2.0 Conference (now
   the Web 2.0 Summit ) was designed to restore confidence in an industry
   that had lost its way after the dotcom bust. The Web was far from done,
   we argued. In fact, it was on its way to becoming a robust platform for
   a culture-changing generation of computer applications and services.

   In our first program, we asked why some companies survived the dotcom
   bust, while others had failed so miserably. We also studied a
   burgeoning group of startups and asked why they were growing so
   quickly. The answers helped us understand the rules of business on this
   new platform.

   Chief among our insights was that "the network as platform" means far
   more than just offering old applications via the network ("software as
   a service"); it means building applications that literally get better
   the more people use them, harnessing network effects not only to
   acquire users, but also to learn from them and build on their
   contributions.

   From Google and Amazon to Wikipedia, eBay, and craigslist, we saw that
   the value was facilitated by the software, but was co-created by and
   for the community of connected users. Since then, powerful new
   platforms like YouTube, Facebook, and Twitter have demonstrated that
   same insight in new ways. Web 2.0 is all about harnessing collective
   intelligence.

   Collective intelligence applications depend on managing, understanding,
   and responding to massive amounts of user-generated data in real time.
   The "subsystems" of the emerging internet operating system are
   increasingly data subsystems: location, identity (of people, products,
   and places), and the skeins of meaning that tie them together. This
   leads to new levers of competitive advantage: Data is the "Intel
   Inside" of the next generation of computer applications.

   Today, we realize that these insights were not only directionally
   right, but are being applied in areas we only imagined in 2004. The
   smartphone revolution has moved the Web from our desks to our pockets.
   Collective intelligence applications are no longer being driven solely
   by humans typing on keyboards but, increasingly, by sensors. Our phones
   and cameras are being turned into eyes and ears for applications;
   motion and location sensors tell where we are, what we’re looking at,
   and how fast we’re moving. Data is being collected, presented, and
   acted upon in real time. The scale of participation has increased by
   orders of magnitude.

   With more users and sensors feeding more applications and platforms,
   developers are able to tackle serious real-world problems. As a result,
   the Web opportunity is no longer growing arithmetically; it’s growing
   exponentially. Hence our theme for this year: Web Squared. 1990-2004
   was the match being struck; 2005-2009 was the fuse; and 2010 will be
   the explosion.

   Ever since we first introduced the term "Web 2.0," people have been
   asking, "What’s next?" Assuming that Web 2.0 was meant to be a kind of
   software version number (rather than a statement about the second
   coming of the Web after the dotcom bust), we’re constantly asked about
   "Web 3.0." Is it the semantic web? The sentient web? Is it the social
   web? The mobile web? Is it some form of virtual reality?

   It is all of those, and more.

   The Web is no longer a collection of static pages of HTML that describe
   something in the world. Increasingly, the Web is the world – everything
   and everyone in the world casts an "information shadow," an aura of
   data which, when captured and processed intelligently, offers
   extraordinary opportunity and mind bending implications. Web Squared is
   our way of exploring this phenomenon and giving it a name.

Redefining Collective Intelligence: New Sensory Input

   To understand where the Web is going, it helps to return to one of the
   fundamental ideas underlying Web 2.0, namely that successful network
   applications are systems for harnessing collective intelligence.

   Many people now understand this idea in the sense of "crowdsourcing,"
   namely that a large group of people can create a collective work whose
   value far exceeds that provided by any of the individual participants.
   The Web as a whole is a marvel of crowdsourcing, as are marketplaces
   such as those on eBay and craigslist, mixed media collections such as
   YouTube and Flickr, and the vast personal lifestream collections on
   Twitter, MySpace, and Facebook.

   Many people also understand that applications can be constructed in
   such a way as to direct their users to perform specific tasks, like
   building an online encyclopedia (Wikipedia), annotating an online
   catalog (Amazon), adding data points onto a map (the many web mapping
   applications), or finding the most popular news stories (Digg, Twine).
   Amazon’s Mechanical Turk has gone so far as to provide a generalized
   platform for harnessing people to do tasks that are difficult for
   computers to perform on their own.

   But is this really what we mean by collective intelligence? Isn’t one
   definition of intelligence, after all, that characteristic that allows
   an organism to learn from and respond to its environment? (Please note
   that we’re leaving aside entirely the question of self-awareness. For
   now, anyway.)

   Imagine the Web (broadly defined as the network of all connected
   devices and applications, not just the PC-based application formally
   known as the World Wide Web) as a newborn baby. She sees, but at first
   she can’t focus. She can feel, but she has no idea of size till she
   puts something in her mouth. She hears the words of her smiling
   parents, but she can’t understand them. She is awash in sensations, few
   of which she understands. She has little or no control over her
   environment.

   Gradually, the world begins to make sense. The baby coordinates the
   input from multiple senses, filters signal from noise, learns new
   skills, and once-difficult tasks become automatic.

   The question before us is this: Is the Web getting smarter as it grows
   up?

   Consider search – currently the lingua franca of the Web. The first
   search engines, starting with Brian Pinkerton’s webcrawler, put
   everything in their mouth, so to speak. They hungrily followed links,
   consuming everything they found. Ranking was by brute force keyword
   matching.

   In 1998, Larry Page and Sergey Brin had a breakthrough, realizing that
   links were not merely a way of finding new content, but of ranking it
   and connecting it to a more sophisticated natural language grammar. In
   essence, every link became a vote, and votes from knowledgeable people
   (as measured by the number and quality of people who in turn vote for
   them) count more than others.

   Modern search engines now use complex algorithms and hundreds of
   different ranking criteria to produce their results. Among the data
   sources is the feedback loop generated by the frequency of search
   terms, the number of user clicks on search results, and our own
   personal search and browsing history. For example, if a majority of
   users start clicking on the fifth item on a particular search results
   page more often than the first, Google’s algorithms take this as a
   signal that the fifth result may well be better than the first, and
   eventually adjust the results accordingly.

   Now consider an even more current search application, the Google Mobile
   Application for the iPhone. The application detects the movement of the
   phone to your ear, and automatically goes into speech recognition mode.
   It uses its microphone to listen to your voice, and decodes what you
   are saying by referencing not only its speech recognition database and
   algorithms, but also the correlation to the most frequent search terms
   in its search database. The phone uses GPS or cell-tower triangulation
   to detect its location, and uses that information as well. A search for
   "pizza" returns the result you most likely want: the name, location,
   and contact information for the three nearest pizza restaurants.

   All of a sudden, we’re not using search via a keyboard and a stilted
   search grammar, we’re talking to and with the Web. It’s getting smart
   enough to understand some things (such as where we are) without us
   having to tell it explicitly. And that’s just the beginning.

   And while some of the databases referenced by the application — such as
   the mapping of GPS coordinates to addresses — are "taught" to the
   application, others, such as the recognition of speech, are "learned"
   by processing large, crowdsourced data sets.

   Clearly, this is a "smarter" system than what we saw even a few years
   ago. Coordinating speech recognition and search, search results and
   location, is similar to the "hand-eye" coordination the baby gradually
   acquires. The Web is growing up, and we are all its collective parents.

Cooperating Data Subsystems

   In our original Web 2.0 analysis, we posited that the future "internet
   operating system" would consist of a series of interoperating data
   subsystems. The Google Mobile Application provides one example of how
   such a data-driven operating system might work.

   In this case, all of the data subsystems are owned by one vendor —
   Google. In other cases, as with Apple’s iPhoto ‘09, which integrates
   Flickr and Google Maps as well as Apple’s own cloud services, an
   application uses cloud database services from multiple vendors.

   As we first noted back in 2003, "data is the Intel Inside" of the next
   generation of computer applications. That is, if a company has control
   over a unique source of data that is required for applications to
   function, they will be able to extract monopoly rents from the use of
   that data. In particular, if a database is generated by user
   contribution, market leaders will see increasing returns as the size
   and value of their database grows more quickly than that of any new
   entrants.

   We see the era of Web 2.0, therefore, as a race to acquire and control
   data assets. Some of these assets — the critical mass of seller
   listings on eBay, or the critical mass of classified advertising on
   craigslist — are application-specific. But others have already taken on
   the characteristic of fundamental system services.

   Take for example the domain registries of the DNS, which are a backbone
   service of the Internet. Or consider CDDB, used by virtually every
   music application to look up the metadata for songs and albums. Mapping
   data from providers like Navteq and TeleAtlas is used by virtually all
   online mapping applications.

   There is a race on right now to own the social graph. But we must ask
   whether this service is so fundamental that it needs to be open to all.

   It’s easy to forget that only 15 years ago, email was as fragmented as
   social networking is today, with hundreds of incompatible email systems
   joined by fragile and congested gateways. One of those systems –
   internet RFC 822 email – became the gold standard for interchange.

   We expect to see similar standardization in key internet utilities and
   subsystems. Vendors who are competing with a winner-takes-all mindset
   would be advised to join together to enable systems built from the
   best-of-breed data subsystems of cooperating companies.

How the Web Learns: Explicit vs. Implicit Meaning

   But how does the Web learn? Some people imagine that for computer
   programs to understand and react to meaning, meaning needs to be
   encoded in some special taxonomy. What we see in practice is that
   meaning is learned "inferentially" from a body of data.

   Speech recognition and computer vision are both excellent examples of
   this kind of machine learning. But it’s important to realize that
   machine learning techniques apply to far more than just sensor data.
   For example, Google’s ad auction is a learning system, in which optimal
   ad placement and pricing is generated in real time by machine learning
   algorithms.

   In other cases, meaning is "taught" to the computer. That is, the
   application is given a mapping between one structured data set and
   another. For example, the association between street addresses and GPS
   coordinates is taught rather than learned. Both data sets are
   structured, but need a gateway to connect them.

   It’s also possible to give structure to what appears to be unstructured
   data by teaching an application how to recognize the connection between
   the two. For example, You R Here, an iPhone app, neatly combines these
   two approaches. You use your iPhone camera to take a photo of a map
   that contains details not found on generic mapping applications such as
   Google maps – say a trailhead map in a park, or another hiking map. Use
   the phone’s GPS to set your current location on the map. Walk a
   distance away, and set a second point. Now your iPhone can track your
   position on that custom map image as easily as it can on Google maps.

   Some of the most fundamental and useful services on the Web have been
   constructed in this way, by recognizing and then teaching the
   overlooked regularity of what at first appears to be unstructured data.

   Ti Kan, Steve Scherf, and Graham Toal, the creators of CDDB, realized
   that the sequence of track lengths on a CD formed a unique signature
   that could be correlated with artist, album, and song names. Larry Page
   and Sergey Brin realized that a link is a vote. Marc Hedlund at Wesabe
   realized that every credit card swipe is also a vote, that there is
   hidden meaning in repeated visits to the same merchant. Mark Zuckerberg
   at Facebook realized that friend relationships online actually
   constitute a generalized social graph. They thus turn what at first
   appeared to be unstructured into structured data. And all of them used
   both machines and humans to do it.

   Key takeaway: A key competency of the Web 2.0 era is discovering
   implied metadata, and then building a database to capture that metadata
   and/or foster an ecosystem around it.

Web Meets World: The "Information Shadow" and the Internet of Things

   Say "sensor-based applications," and many people might imagine a world
   of applications driven by RFID tags or ZigBee modules. This future is
   conveniently far off, with test deployments and a few exciting early
   stage applications. But what many people fail to notice is how far
   along the sensor revolution already is. It’s the hidden face of the
   mobile market, and its most explosive opportunity.

   Today’s smartphones contain microphones, cameras, motion sensors,
   proximity sensors, and location sensors (GPS, cell-tower triangulation,
   and even in some cases, a compass). These sensors have revolutionized
   the user interface of standalone applications — you have only to play
   with Smule’s Ocarina for the iPhone to see that.

   But remember: mobile applications are connected applications. The
   fundamental lessons of Web 2.0 apply to any network application,
   whether web- or mobile phone-based (and the lines between the two are
   increasingly blurred). Sensor-based applications can be designed to get
   better the more people use them, collecting data that creates a
   virtuous feedback loop that creates more usage. Speech recognition in
   Google Mobile App is one such application. New internet-connected GPS
   applications also have built-in feedback loops, reporting your speed
   and using it to estimate arrival time based on its knowledge of traffic
   ahead of you. Today, traffic patterns are largely estimated;
   increasingly, they will be measured in real time.

   The Net is getting smarter faster than you might think. Consider
   geotagging of photos. Initially, users taught their computers the
   association between photos and locations by tagging them. When cameras
   know where they are, every photo will be geotagged, with far greater
   precision than the humans are likely to provide.

   And the increased precision in one data set increases the potential of
   another. Consider the accuracy of these maps generated by geotagged
   Flickr photos:

   Flickr geotag map of USA
   Flickr geotag map of Texas

   How much more accurate will these maps be when there are billions of
   photos?

   Nor will the training wheels for the Net’s visual sensor network be
   limited to location.

   It’s still in its early days, but the face recognition in Apple iPhoto
   ‘09 is pretty good. At what point are enough faces tagged with names
   that the system is able to show you only the people it doesn’t
   recognize? (Whether or not Apple imagines providing this data as a
   system service is an open question; whether someone else does it as a
   network service is assuredly not.)

   The Wikitude travel guide application for Android takes image
   recognition even further. Point the phone’s camera at a monument or
   other point of interest, and the application looks up what it sees in
   its online database (answering the question "what looks like that
   somewhere around here?") The screen shows you what the camera sees, so
   it’s like a window but with a heads-up display of additional
   information about what you’re looking at. It’s the first taste of an
   "augmented reality" future. It superimposes distances to points of
   interest, using the compass to keep track of where you’re looking. You
   can sweep the phone around and scan the area for nearby interesting
   things.

   Layar takes this idea even further, promising a framework for multiple
   layers of "augmented reality" content accessed through the camera of
   your mobile phone.

   Think of sensor-based applications as giving you superpowers. Darkslide
   gives you super eyesight, showing you photos near you. iPhone Twitter
   apps can "find recent tweets near you" so you can get super hearing and
   pick up the conversations going on around you.

Photosynth, Gigapixel Photography, and Infinite Images

   The increasing richness of both sensor data and machine learning will
   lead to new frontiers in creative expression and imaginative
   reconstruction of the world.

   Microsoft’s Photosynth demonstrates the power of the computer to
   synthesize 3D images from crowdsourced photographs. Gigapixel
   photography reveals details that were invisible even to people on the
   scene. Adobe’s Infinite Images reveals something even more startling:
   the ability of the computer to synthesize imaginary worlds that never
   existed, extrapolating a complete 3D experience from a set of photos.
   The video demonstration needs to be seen to be believed.

   All of these breakthroughs are reflections of the fact noted by Mike
   Kuniavsky of ThingM, that real world objects have "information shadows"
   in cyberspace. For instance, a book has information shadows on Amazon,
   on Google Book Search, on Goodreads, Shelfari, and LibraryThing, on
   eBay and on BookMooch, on Twitter, and in a thousand blogs.

   A song has information shadows on iTunes, on Amazon, on Rhapsody, on
   MySpace, or Facebook. A person has information shadows in a host of
   emails, instant messages, phone calls, tweets, blog postings,
   photographs, videos, and government documents. A product on the
   supermarket shelf, a car on a dealer’s lot, a pallet of newly mined
   boron sitting on a loading dock, a storefront on a small town’s main
   street — all have information shadows now.

   In many cases, these information shadows are linked with their real
   world analogues by unique identifiers: an ISBN or ASIN, a part number,
   or getting more individual, a social security number, a vehicle
   identification number, or a serial number. Other identifiers are
   looser, but identity can be triangulated: a name plus an address or
   phone number, a name plus a photograph, a phone call from a particular
   location undermining what once would have been a rock-solid alibi.

   Many who talk about "the Internet of Things" assume that what will get
   us there is the combination of ultra-cheap RFID and IP addresses for
   everyday objects. The assumption is that every object must have a
   unique identifier for the "Internet of Things" to work.

   What the Web 2.0 sensibility tells us is that we’ll get to the Internet
   of Things via a hodgepodge of sensor data contributing, bottom-up, to
   machine-learning applications that gradually make more and more sense
   of the data that is handed to them. A bottle of wine on your
   supermarket shelf (or any other object) needn’t have an RFID tag to
   join the "Internet of Things," it simply needs you to take a picture of
   its label. Your mobile phone, image recognition, search, and the
   sentient web will do the rest. We don’t have to wait until each item in
   the supermarket has a unique machine-readable ID. Instead, we can make
   do with bar codes, tags on photos, and other "hacks" that are simply
   ways of brute-forcing identity out of reality.

   There’s a fascinating fact noted by Jeff Jonas in his work on identity
   resolution. Jonas’ work included building a database of known US
   persons from various sources. His database grew to about 630 million
   "identities" before the system had enough information to identify all
   the variations. But at a certain point, his database began to learn,
   and then to shrink. Each new load of data made the database smaller,
   not bigger. 630 million plus 30 million became 600 million, as the
   subtle calculus of recognition by "context accumulation" worked its
   magic.

   As the information shadows become thicker, more substantial, the need
   for explicit metadata diminishes. Our cameras, our microphones, are
   becoming the eyes and ears of the Web, our motion sensors, proximity
   sensors its proprioception, GPS its sense of location. Indeed, the baby
   is growing up. We are meeting the Internet, and it is us.

   Sensors and monitoring programs are not acting alone, but in concert
   with their human partners. We teach our photo program to recognize
   faces that matter to us, we share news that we care about, we add tags
   to our tweets so that they can be grouped more easily. In adding value
   for ourselves, we are adding value to the social web as well. Our
   devices extend us, and we extend them.

   Nor is this phenomenon limited to the consumer web. IBM’s Smarter
   Planet initiative and the NASA-Cisco "planetary skin" project both show
   how deeply business will be transformed by the sensor web. Oil
   refineries, steel mills, factories, and supply chains are being
   instrumented with sensors and exactly the same kind of machine learning
   algorithms that we see in web applications.

   But as is so often the case, the future isn’t clearest in the
   pronouncements of big companies but in the clever optimizations of
   early adopters and "alpha geeks." Radar blogger Nat Torkington tells
   the story of a taxi driver he met in Wellington, NZ, who kept logs of
   six weeks of pickups (GPS, weather, passenger, and three other
   variables), fed them into his computer, and did some analysis to figure
   out where he should be at any given point in the day to maximize his
   take. As a result, he’s making a very nice living with much less work
   than other taxi drivers. Instrumenting the world pays off.

   Data analysis, visualization, and other techniques for seeing patterns
   in data are going to be an increasingly valuable skillset. Employers
   take notice.

   This isn’t to say that there isn’t a huge role for unique identifiers
   for objects, especially fungible objects that are instances of a
   well-known class (like a book or music collection). But evidence shows
   that formal systems for adding a priori meaning to digital data are
   actually less powerful than informal systems that extract that meaning
   by feature recognition. An ISBN provides a unique identifier for a
   book, but a title + author gets you close enough.

   Projects to systematically categorize raw sensor data may be created,
   along the lines of the Astrometry project, whose founders claim, "We
   are building an ‘astrometry engine’ to create correct,
   standards-compliant astrometric meta data for every useful astronomical
   image ever taken, past and future, in any state of archival disarray."
   Using this engine, the Flickr astrotagger bot trolls Flickr for images
   of astronomical objects and gives them proper metadata, which then
   allows them to be included in astronomical image search by name. This
   is a service directly analogous to CDDB: a lookup service that maps
   messy sensor data to a regularized lookup database.

   As is often the case, the early examples are often the work of
   enthusiasts. But they herald a world in which entrepreneurs apply the
   same principles to new business opportunities. As more and more of our
   world is sensor-enabled, there will be surprising revelations in how
   much meaning – and value — can be extracted from their data streams.

   Consider the so-called "smart electrical grid." Gavin Starks, the
   founder of AMEE, a neutral web-services back-end for energy-related
   sensor data, noted that researchers combing the smart meter data from
   1.2 million homes in the UK have already discovered that each device in
   the home has a unique energy signature. It is possible to determine not
   only the wattage being drawn by the device, but the make and model of
   each major appliance within – think CDDB for appliances and consumer
   electronics!

   Mapping from unstructured data to structured data sets will be a key
   Web Squared competency.

The Rise of Real Time: A Collective Mind

   As it becomes more conversational, search has also gotten faster.
   Blogging added tens of millions of sites that needed to be crawled
   daily or even hourly, but microblogging requires instantaneous update –
   which means a significant shift in both infrastructure and approach.
   Anyone who searches Twitter on a trending topic has to be struck by the
   message: "See what’s happening right now" followed, a few moments later
   by "42 more results since you started searching. Refresh to see them."

   What’s more, users are continuing to co-evolve with our search systems.
   Take hashtags on Twitter: a human convention that facilitates real-time
   search on shared events. Once again, you see how human participation
   adds a layer of structure – rough and inconsistent as it is – to the
   raw data stream.

   Real-time search encourages real-time response. Retweeted "information
   cascades" spread breaking news across Twitter in moments, making it the
   earliest source for many people to learn about what’s just happened.
   And again, this is just the beginning. With services like Twitter and
   Facebook’s status updates, a new data source has been added to the Web
   – realtime indications of what is on our collective mind.

   Guatemala and Iran have both recently felt the Twitter effect, as
   political protests have been kicked off and coordinated via Twitter.

   Which leads us to a timely debate: There are many who worry about the
   dehumanizing effect of technology. We share that worry, but also see
   the counter-trend, that communication binds us together, gives us
   shared context, and ultimately shared identity.

   Twitter also teaches us something important about how applications
   adapt to devices. Tweets are limited to 140 characters; the very limits
   of Twitter have led to an outpouring of innovation. Twitter users
   developed shorthand (@username, #hashtag, $stockticker), which Twitter
   clients soon turned into clickable links. URL shorteners for
   traditional web links became popular, and soon realized that the
   database of clicked links enable new real-time analytics. Bit.ly, for
   example, shows the number of clicks your links generate in real time.

   As a result, there’s a new information layer being built around Twitter
   that could grow up to rival the services that have become so central to
   the Web: search, analytics, and social networks. Twitter also provides
   an object lesson to mobile providers about what can happen when you
   provide APIs. Lessons from the Twitter application ecosystem could show
   opportunities for SMS and other mobile services, or it could grow up to
   replace them.

   Real-time is not limited to social media or mobile. Much as Google
   realized that a link is a vote, WalMart realized that a customer
   purchasing an item is a vote, and the cash register is a sensor
   counting that vote. Real-time feedback loops drive inventory. WalMart
   may not be a Web 2.0 company, but they are without doubt a Web Squared
   company: one whose operations are so infused with IT, so innately
   driven by data from their customers, that it provides them immense
   competitive advantage. One of the great Web Squared opportunities is
   providing this kind of real-time intelligence to smaller retailers
   without monolithic supply chains.

   As explained so eloquently by Vivek Ranadive, founder and CEO of Tibco,
   in Malcolm Gladwell’s recent New Yorker profile:

   "Everything in the world is now real time. So when a certain type of
   shoe isn’t selling at your corner shop, it’s not six months before the
   guy in China finds out. It’s almost instantaneous, thanks to my
   software."

   Even without sensor-driven purchasing, real-time information is having
   a huge impact on business. When your customers are declaring their
   intent all over the Web (and on Twitter) – either through their actions
   or their words, companies must both listen and join the conversation.
   Comcast has changed its customer service approach using Twitter; other
   companies are following suit.

   Another striking story we’ve recently heard about a real-time feedback
   loop is the Houdini system used by the Obama campaign to remove voters
   from the Get Out the Vote calling list as soon as they had actually
   voted. Poll watchers in key districts reported in as they saw names
   crossed off the voter lists; these were then made to "disappear" from
   the calling lists that were being provided to volunteers. (Hence the
   name Houdini.)

   Houdini is Amazon’s Mechanical Turk writ large: one group of volunteers
   acting as sensors, multiple real-time data queues being synchronized
   and used to affect the instructions for another group of volunteers
   being used as actuators in that same system.

   Businesses must learn to harness real-time data as key signals that
   inform a far more efficient feedback loop for product development,
   customer service, and resource allocation.

In Conclusion: The Stuff that Matters

   All of this is in many ways a preamble to what may be the most
   important part of the Web Squared opportunity.

   The new direction for the Web, its collision course with the physical
   world, opens enormous new possibilities for business, and enormous new
   possibilities to make a difference on the world’s most pressing
   problems.

   There are already hundreds of examples of this happening. But there are
   many other areas in which we need to see a lot more progress – from our
   energy ecosystem to our approach to healthcare. Not to mention our
   financial system, which is in disarray. Even in a pro-regulatory
   environment, the regulators in government are hopelessly outclassed by
   real-time automated financial systems. What have we learned from the
   consumer internet that could become the basis for a new 21st century
   financial regulatory system? We need machine learning to be applied
   here, algorithms to detect anomalies, transparency that allows auditing
   by anyone who cares, not just by overworked understaffed regulators.

   When we started the Web 2.0 events, we stated that "the Web is a
   platform." Since then, thousands of businesses and millions of lives
   have been changed by the products and services built on that platform.

   But 2009 marks a pivot point in the history of the Web. It’s time to
   leverage the true power of the platform we’ve built. The Web is no
   longer an industry unto itself – the Web is now the world.

   And the world needs our help.

   If we are going to solve the world’s most pressing problems, we must
   put the power of the Web to work – its technologies, its business
   models, and perhaps most importantly, its philosophies of openness,
   collective intelligence, and transparency. And to do that, we must take
   the Web to another level. We can’t afford incremental evolution
   anymore.

   It’s time for the Web to engage the real world. Web meets World –
   that’s Web Squared.

A Call For Examples!

   As part of this paper and our work on the agenda for the Web 2.0
   Summit, we’d like your input. We’re looking to create a list of
   applications, services, and projects that reflect the Web Squared
   theme. A few examples:

   - The election of Barack Obama has demonstrated how the Internet can be
   used to transform politics. Now, his administration is committed to
   exploring how it might be used to transform the actual practice of
   governing.

   The US Federal government has made a major commitment to transparency
   and open data. Data.gov now hosts more than 100,000 data feeds from US
   government sources, and the White House blog is considering a
   commitment to the 8 Open Data Principles articulated by a group of open
   data activists in late 2007. There’s a celebration of the successes
   that many are now calling "Government 2.0." We’d love to hear about
   Government 2.0 success stories from around the world.

   But in his advice on the direction of the Government 2.0 Summit Federal
   CTO Aneesh Chopra has urged us not to focus on the successes of Web 2.0
   in government, but rather on the unsolved problems. How can the
   technology community help with such problems as tracking the progress
   of the economic stimulus package in creating new jobs? How can it speed
   our progress towards energy independence and a reduction in CO2
   emissions? How can it help us remake our education system to produce a
   more competitive workforce? How can it help us reduce the ballooning
   costs of healthcare?

   - Twitter is being used to report news of disasters, and to coordinate
   emergency response. Initiatives like Instedd (Innovative Support to
   Emergencies, Diseases, and Disasters) take this trend and amp it up.
   Instedd uses collective intelligence techniques to mine sources like
   SMS messages (e.g., Geochat), RSS feeds, email lists (e.g., ProMed,
   Veratect, HealthMap, Biocaster, EpiSpider), OpenROSA, Map Sync, Epi
   Info™, documents, web pages, electronic medical records (e.g.,
   OpenMRS), animal disease data (e.g., OIE, AVRI hotline), environmental
   feed, (e.g., NASA remote sensing, etc.) for signals of emerging
   diseases.

   The Global Virus Forecasting Initiative (GVFI) now deliberately
   collects data (in this case, about emerging diseases crossing over from
   animal to human) that can be fed into this global early warning system.

   - Our health care system is tottering. Meanwhile, there is little
   correlation between spending and outcomes. As Atul Gawande wrote in the
   New Yorker:

   "Local executives for hospitals and clinics and home-health agencies
   understand their growth rate and their market share; they know whether
   they are losing money or making money. They know that if their doctors
   bring in enough business—surgery, imaging, home-nursing referrals—they
   make money; and if they get the doctors to bring in more, they make
   more. But they have only the vaguest notion of whether the doctors are
   making their communities as healthy as they can, or whether they are
   more or less efficient than their counterparts elsewhere."

   In short, we’re measuring the wrong things. How do we apply the lessons
   of Web 2.0 to measure the right things about healthcare?

   - Companies like 23andMe and PatientsLikeMe are applying crowdsourcing
   to build databases of use to the personalized medicine community.
   23andMe provides genetic testing for personal use, but their long term
   goal is to provide a database of genetic information that members could
   voluntarily provide to researchers. PatientsLikeMe has created a social
   network for people with various life-changing diseases; by sharing
   details of treatment – what’s working and what’s not – they are in
   effect providing a basis for the world’s largest longitudinal medical
   outcome testing service. What other creative applications of Web 2.0
   technology are you seeing to advance the state of the art in
   healthcare?

   - How do we create economic opportunities in reducing the cost of
   healthcare? As Stanford’s Abraham Verghese writes, the reason it’s so
   hard to cut healthcare costs is that "a dollar spent on medical care is
   a dollar of income for someone." We can’t just cut costs. We need to
   find ways to make money by cutting costs. In this regard, we’re
   intrigued by startups like CVsim, a cardio-vascular simulation company.
   Increasingly accurate data from CAT scans, coupled with blood flow
   simulation software running on a cloud platform, makes it conceivable
   to improve health outcomes and reduce costs while shrinking a
   multi-billion dollar market for angiography, an expensive and risky
   medical procedure. If CVsim succeeds in this goal, they’ll build a huge
   company while shrinking the nation’s healthcare bill. What other
   similar opportunities are there for technology to replace older, less
   effective medical procedures with newer ones that are potentially more
   effective while costing less?

   - As part of the financial stimulus package, the government is spending
   $5 billion dollars on weatherization subsidies. How might Web 2.0
   technologies tell us if the program is meeting its goal of creating
   jobs and reducing energy usage?

   - Forward looking companies are adopting real-time monitoring and
   management to build smarter supply chains, manage remote resources, and
   in general, improve their return on investment using what Doug Standley
   at Deloitte calls "Asset Intelligence." We’d love to hear examples from
   people who are deploying these technologies.

   - Real-time traffic monitoring systems like Microsoft Clearflow reduce
   wasted time and energy commuting. Web services reporting progress of
   buses and trains against their scheduled times make public transit more
   effective and enjoyable. These are tangible consumer benefits from
   instrumenting the world. Sensor-driven congestion pricing schemes like
   the one IBM built for the city of Stockholm create economic incentives
   to reduce traffic at peak times. These initiatives also raise privacy
   issues. We’re interested in hearing about success stories – and scare
   stories – about the way that instrumenting the world changes the way we
   live.

   - Smart Grid initiatives will reduce our energy usage by increasing the
   intelligence of the system used to deliver it. As hinted at above,
   though, they will also open a whole new front in the war on privacy.
   The data that will be revealed by smart grid applications will not only
   make our utilities smarter, it will likely make marketers a lot smarter
   too. It is unlikely, though, to make them more humane and less
   intrusive!

   Disclosure: Via our venture investing arm, O’Reilly AlphaTech Ventures,
   O’Reilly is an investor in three of the companies mentioned in this
   article: Wesabe, AMEE, and bit.ly.

   Comments on this page are now closed.

Comments

   Jamie Thompson
   10/24/2009 12:27pm PDT

   Pongr is productizing its computer vision research around object
   recognition from mobile users. For examples of image searchable
   objects, see www.pongr.com or our blog. There are current real-world
   uses for mobile marketing campaigns, but much more to come when the
   greater world of “things” starts getting recognizable.
   Michele Lin
   10/20/2009 3:41pm PDT

   Very fascinating article on how the web has evolved and it’s future!
   Maximilian Harmon
   10/20/2009 10:46am PDT

   Awesome article! I really appreciate the way that O’Reilly and Battle
   have the foresight and experience to be able to predict or at least
   make some very educated guesses about the future of the web in 5 yrs.
   michael kelly
   10/20/2009 10:04am PDT

   i really enjoyed this article!
   giulio quaggiotto
   10/19/2009 1:11pm PDT

   Great article. Very inspirational. In my view, the section on “stuff
   that matters” can be expanded even further, for example applying it to
   the international development/aid sector. I am thinking for example of
   disaster risk management or post-conflict nations. I posted some
   examples, based on the WebSquared framework here (bit.ly/2v3qem).
   David Morse
   10/18/2009 11:05am PDT

   I definitely understand the concerns about web privacy. However, I also
   understand the significance of collective intelligence. I know we’re
   all familiar with the saying, “two heads are better than one.” The next
   technological breakthroughs will result from collaboration and
   effective interactive conservation.
   Matt Perez
   10/15/2009 6:56pm PDT

   Great characterization of where we are and where we are going. I would
   not want to see this be held back in any way. So, please, before you
   really push “Web Squared” out there, think of the hash tag. I am not
   being facetious.

   In no small part, the acceptance of “Web 2.0” as a short-hand for the
   paradigm change we’ve been through in the last few years was helped in
   large part by its succinctness. I don’t think it would have been
   accepted as widely, as quickly if it had been called “The Next
   Generation Web” or “The Web Super-Duper-Highway.”

   What would Web Squared look like in a Twitter post (the extreme case)?
   #websquared? 11 characters? forget it! #web2? too confusing, too close
   to #web2? Oops, that’s already been taken to mean ‘Web 2.0.’ Then,
   what, #web^2? There are about 37 people in the world who know where the
   ^ key is, so that’s not going to work, either.

   Also, “Squared” does not have the emotional hook that “2.0” had from
   the very start. And to top it off, the words don’t quite roll off the
   tongue. “Hey, so it that an Enterprise Squared kinda product?” I don’t
   think so.

   Again, I am not being critical and if anything I am trying to “further
   the cause” by pointing out a glaring weakness.

   I can’t think of an alternative right now (I am still digesting the
   article), but maybe we ought to crowdsource this question?

   BTW, I was really surprised that Google Wave was mentioned only once,
   briefly. I believe that that will be the technology that will end up
   most closely associated with “Web… er… Squared;” its poster child, so
   to speak.

   Not as a plug, but because it is complementary to this discussion, I’ve
   just written a post that fleshes out the point, Wave is to Interaction
   as the Browser was to Access
   Picture of Tom Crowl
   Tom Crowl
   10/12/2009 7:31am PDT

   Thanks for this great analysis!

   I ran across a bit of useful data which for me illustrates a continuing
   issue in which I believe the Internet has a vital role… A quote from
   Lawrence Lessig in a piece the other day…

   “Each legislator voting Yes … received an average of $37,700 from the
   Oil & Gas, Coal Mining and Nuclear Energy industries between 2003 and
   2008, more than three times as much as the $11,304 received by each
   legislator voting No.”

   Citizens per Congressional District in 2008= 700,000

   So on both sides of that issue total about $10,000 a year for each
   districts representative or about 7 cents per constituent per year.

   Regardless of one’s position on this or any other issue, the point
   being made regards a persistent imbalance in meaningful influence
   capability.

   For better or worse, monetary political contribution is recognized as
   an element of speech.

   Issues of both scale and technology have (unintentionally) fed this
   imbalance in influence. The microtransaction is certainly an area of
   interest.

   But the Political MicroContribution is both vital and inevitable*. The
   potentials of networked citizen lobbying will soon be recognized in
   both ad hoc and more formal associations.

   The characteristics of the platform hosting such transactions have
   important implications which merit discussion. (For instance if such
   capability were facilitated through separate partisan sites which is of
   obvious advantage to political parties which essentially bundle
   opinions… but disastrous for representative government since for the
   individual it inhibits granularity of opinion and encourages
   stagnation, group-think, and the degradation of debate.)

   It’s also likely public funding of elections will be both more
   acceptable and equitable were such funds distributed directly to voters
   for their own individual contributions to legitimate candidates whether
   for primary or general elections.

   Further the transaction must not be weighted with additional
   transaction costs and governance of the platform must be transparent
   and participatory under Enlightenment principals.

   Democracy is risky and difficult and warrants continuous attention
   regarding its designs and technologies. Having a well running
   civilization certainly involves a bit of luck. But to neglect conscious
   design in a world facing a Moore’s law of cultural change is foolhardy.

   Chagora
   dionicio ramirez
   10/06/2009 11:30am PDT

   Web 2.0 has come a long way and continues to make ground breaking
   advancements. With web 2.0 and others like it the future will create
   completely new verison of what we call the world wide web!
   Gino Morena
   10/06/2009 11:03am PDT

   First off, i love th comparison to the new born baby and this type of
   technology getting aquainted with what exists. The grid is smething i
   heard about in high school but its cool to see t come to fruition.
   jacob bautista
   10/06/2009 10:44am PDT

   The only thing that concerns me about the evolution of the web is the
   point of privacy. The more that people are connected and collaborating,
   the more chances there are for those to do harm. Maybe I’m just a
   little paranoid.
   Michael Colello
   10/06/2009 10:40am PDT

   It seems to me that things like the smart electrical grid are more and
   more going to become the norm in today’s digital landscape. Especially
   when we consider the disasters that have happened in the past in this
   area.
   Elle Robinson
   10/06/2009 10:09am PDT

   In response to the articles section “Photosynth, Gigapixel Photography,
   and Infinite Images” is a section of Web 2.0 that has saved some
   Americans during this economic downturn. Web 2.0 has made digital art
   available to anyone who is willing to learn it. The HD quality that
   Youtube video’s have is great because it is setting the standards high
   for the quality of images on the web and potentially the quality of
   content on the web.
   Stevie Calderon
   10/04/2009 3:17pm PDT

   It has been interesting to see the growth Web 2.0 has had over the past
   couple of years. I am looking forward to seeing how it continues to
   expand and grow after seeing websites such as Facebook and YouTube
   explode like they did. There is always room for improvement and it
   seems like Web 2.0 is heading in the right direction.
   Benjamin Wells
   09/28/2009 9:46pm PDT

   In my view, discussion is half the prize, half the battle, half the
   problem, half the reward. It should be more. We need web facilities
   that mediate discussion better. For example, I don’t think articles +
   comments (or better methods now in use) will be seen as paradigmatic in
   a few years or Web n.0 generations.
   Benjamin Ogden
   08/25/2009 11:13am PDT

   Excellent post. At thoughts.com we are committed to making a positive
   and lasting impact on the world. I’m really looking forward to this
   year’s summit and the opportunity to our OPEN dinner.
   Michal Piorkowski
   08/24/2009 7:21am PDT

   Concerning the call for examples, I’d like to draw your attention to
   what’s going on in Sweden – they’re going to have first city-wide
   ZigBee deployment. More details here: www.zigbee.org/imwp/idms/po...
   Christopher Barnatt
   08/13/2009 1:45pm PDT

   I think this is a great paper. Like many others I’ve never been very
   happy with the idea of just “Web 3.0” following Web 2.0, as whatever
   label takes us beyond Web 2.0 I think has to signal how the human race
   embraces the smart device cloud not just technologically but
   culturally. I therefore “clicked” with your paper straight away and
   immediately made a video! See www.youtube.com/watch?v=RWx...
   Clark Benson
   08/08/2009 4:43pm PDT

   Great post. Along these lines: , I have also just launched an ambitious
   site, www.ranker.com, in our case turning “Rankings” of anything into
   “votes”. Everything in our system is an object so that we can derive
   value/knowledge/correlations from all connections. UI still needs a bit
   of honing but i think we fit right into this ecosystem.
   Richard Crews
   08/06/2009 7:52pm PDT

   Brilliant, Tim, thanks. Key idea: We are each and all beginners AND
   sophisticated users (and always will be—at various times, in various
   ways); therefore, user-assisted interfaces are important so that the
   slowest among us (like me) can access effortlessly the most advanced
   information/knowledge technologies. Smoothing and simplifying user
   interfacing is part of the web^2 evolution. Key idea: Social issues
   rule; for example, (1) there is enough food in the world but crippled
   access/distribution leaves a billion people hungry; (2) can the U.S.’s
   incarcerated minions be taught fundamental, mini-avenues of IT to
   enable them to contribute and avoid recidivism? (3) etc., i.e., social
   problems intersect with web squaring.
   Kel Kelly
   08/03/2009 10:02am PDT

   tim, i have been waiting for you to clearly articulate web squared and
   feel a tremendous sense of relief to have it. your brilliance never
   ceases to amaze me. i love playing in this sandbox and i’m stoked to
   see its evolution. peace out.
   Mark Drummond
   07/30/2009 12:10pm PDT

   Excellent high-level analysis of where the web is, and where it might
   go.

   We at Wowd (www.wowd.com) agree that the big opportunities are all
   about harnessing collective intelligence.

   I won’t turn this into an ad for what we’re doing, but will just say
   that we’re building a search, discovery, and recommendation tool that’s
   based on exactly this idea.

   A more detailed response to the Web Squared paper can be found here:
   blog.wowd.com/wowd_blog/200...
   Mike McCamon
   07/23/2009 8:46pm PDT

   We are developing an open source initiative to bring transparency to
   the non-profit safe water sector. This system would not simply map
   water projects in the developing world but provide real-time project
   status information to those looking to follow their charitable gifts.
   As NGOs get pressure to deliver ever-increasing beneficiary numbers
   many are double and triple-counted. By building a sector-wide
   monitoring system we will be able to demonstrate validated progress on
   solving the safe water and sanitation crisis.
   Sumeet Anand
   07/15/2009 10:17am PDT

   societal transformation and sustainable living by empowering the
   collective is the need…Web need to have a larger cause and meaning
   beyond just entertainment tinyurl.com/mf947g
   mez breeze
   07/07/2009 5:03pm PDT

   Instead of employing the term “Web Squared” to describe such emergent
   real-time user-generated states, I prefer the term Social Tesseracting.
   This concept is a tad more elastic and describes the altered
   dimensional aspects of this radical change in human engagement.

   Several of the markers of Social Tesseracting include:

   1. Social White-Space: “Social white space exists in synthetically
   mediated consciousness via overlaying reality clusters. These clusters
   may exist outside of the geoloaded end of the Reality-Virtuality
   Continuum [ie the locatable “real person”]. Conjunctive or intermediary
   areas of connectivity mediate this “primary” reality state [think:
   Information Shadowing, the Network Effect and Warnock’s Dilemma].”

   2. Immediation

   3. Regenerative Comprehension: “indicated by rapid shifts in the nature
   of content creation and absorption. A primary example is Twitter’s
   chronologically-reversed tweet reading order acting to modify
   awareness. Other examples include: + Institutionalised settings
   validating abbreviated textspeak. + Gradual modification of
   standardised literacy conventions [think: seamless acceptance of
   typographical errors and upper and lower case montaging]. + Real-time
   lifestreaming effecting established cognition patterns [think: Active
   Narrative Gathering in Social Games]. Aggregated lifesharing also
   influences user-generated functionality shifts [think: communication
   workarounds].”

   4. Process Centering

   5. Information Deformation: “Social Tesseractions are marked by fluid,
   process-oriented engagement rather than rigid procedural structuring.
   Process centering prompts a re-evaluation of data formation and alters
   the entrenched importance of institutionalised categorisations. An
   emergent example of process centering is Google Wave. Google Wave uses
   an algorithmic variation of “operational transformations” [live
   concurrent editing] which occur through a process called
   transformation…”

   6. Attribution Modding

   7. Decline of Silo Ghettos

   Social Tesseraction is described in full at Augmentology 1[L]0[L]1 – a
   project that discusses ”...the formation and evolution of synthetic
   environments.”

   Regards, Mez Breeze
   Mark Graham
   07/05/2009 12:43pm PDT

   In response to your call for examples, the Wikichains project that aims
   to harness cloud collaboration in order to map all nodes of all
   commodity chains may one day be another substantial link between the
   on- and off-line worlds.
   Tarun Anand
   07/04/2009 7:53am PDT

   I firmly believe what Tim is saying here will come true. Interestingly
   what will happen is that in some emerging markets the access mediums
   and so called sensors will be inverted. The mobile and related
   technologies will provide more data than web, but both will happily
   co-exist and create the web squared. We at mScriber (www.mscriber.com)
   want to create and let people access this information “stream” using
   voice.

   Regards,

   Tarun
   Jim Mortleman
   07/02/2009 3:09am PDT

   (Gah -I’m not all with it this morning! One last try…

   bit.ly/CTXiQ
   Jim Mortleman
   07/02/2009 2:58am PDT

   Thanks, Tim, for such a riveting round-up of the evolving web. One
   great example of how the way we interact with real-world objects could
   evolve in this landscape is presented in Pattie Maes’s much-retweeted
   TED demo of the ‘Sixth Sense’ project, which uses low-cost portable cam
   and projector technology to offer users a ‘Minority Report’-style
   augmented reality: bit.ly/CTXiQ.
   >
   Gene Golovchinsky
   06/30/2009 7:34am PDT

   You describe an interesting and thought-provoking vision for how
   computer-mediated communication can improve our society. It would also
   be interesting to discuss how to prevent various spam mechanisms from
   undermining the potential of this medium. See my post If you built it,
   they will spam .
   Fabrice Florin
   06/29/2009 9:31pm PDT

   Thanks for this little gem! You’ve written a truly visionary paper,
   which sets a bold yet realistic agenda for innovations to come. It has
   already expanded my horizon. Excellent work.
   Picture of Steffen Konrath
   Steffen Konrath
   06/28/2009 1:51am PDT

   Great article which I will recommend to my followers on Twitter as
   well.
   Gui Ambros
   06/25/2009 9:55pm PDT

   Brilliant masterpiece. I loved how elegantly and eloquently you
   described that our world IS the platform.
   Kristiono Setyadi
   06/25/2009 1:48pm PDT

   I hope we can see the new face of this “baby” in not more than three
   years from now :)

Schedule views

     * Tue 20 Oct
     * Wed 21 Oct
     * Thu 22 Oct
     * Full listing
     * Personal schedule

   HOME  |  O'REILLY CONFERENCES |  TECHWEB  |  PRIVACY POLICY |  CONTACT
   US |  SITEMAP
   O'Reilly Media Logo TechWeb Logo

   Co-produced By O'Reilly Media, Inc. and TechWeb
   © 2009, O'Reilly Media, Inc. and TechWeb
   (707) 827-7000 / (800) 998-9938
   conf-webmaster@oreilly.com
   Event Software Powered by Expectnation
   Co-produced By:
   O'Reilly Media Logo techweb.com
   Diamond Sponsor
     * SAP

   Platinum Sponsors
     * Juniper Networks
     * Microsoft Corporation
     * Ovi by Nokia

   Gold Sponsors
     * Intel
     * MySpace.com
     * Thomson Reuters
     * Yahoo! Inc.

   Silver Sponsor
     * mental images

   Bronze Sponsors
     * Answers.com
     * Atigeo
     * Unity Media Group

   High Order Ignite Sponsor
     * Omidyar Network

   Media Sponsors
     * The Business Insider
     * Government Executive Media Group
     * Mashable
     * Nextgov
     * Topix.net
     * VentureBeat

   Sponsor Opportunities

   Reach business leaders and technology influencers at the Web 2.0
   Summit. Call Rob Koziura at (415) 947-6111 or email
   rkoziura@techweb.com.
   Contact Us

   View a complete list of Web 2.0 Summit contacts.
